# Taken from:
# https://github.com/troycomi/snakemake-training/blob/1b2861ccc52bda56c0838cdd5b6f55c23fc16e2e/princeton_rc/config.yaml

---
# basic configuration
use-singularity: true
use-conda: true
conda-frontend: conda
printshellcmds: true

# these control where the images are stored.  If you have a central location
# you don't have to remake images when changing working directory
# Alternatively you can set the shadow-prefix for the entire .snakemake dir
singularity-prefix: "~/snakemake_images"
conda-prefix: "~/snakemake_images"

# cluster specific settings
cluster: "sbatch --cpus-per-task={threads} --mem={resources.mem_mb}M \
            --time={resources.runtime} --output=slurm_out/%x-%A \
            --job-name={rule} --parsable"
cluster-status: "slurm-status.py"
cluster-cancel: scancel
cluster-cancel-nargs: 50
latency-wait: 120  # wait 2 minutes for missing files before raising exception
                   # important for NFS
jobs: 250  # maximum jobs to run at once
max-jobs-per-second: 1
max-status-checks-per-second: 10
local-cores: 4  # maximum local jobs to run
resources: short_jobs=2  # custom resources, prevents tying up queue with short
                         # qos jobs